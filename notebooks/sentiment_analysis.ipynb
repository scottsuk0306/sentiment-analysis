{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8506376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path + \"\\src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f50a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import normalize_text\n",
    "import pandas as pd\n",
    "\n",
    "imdb_data=pd.read_csv('../data/normalized IMDB dataset.csv')\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b24a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (40000,)\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#split the dataset  \n",
    "#train dataset\n",
    "train_reviews=imdb_data.review[:40000]\n",
    "train_sentiments=imdb_data.sentiment[:40000]\n",
    "#test dataset\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b86f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "# nltk.download(stopwords)\n",
    "#imdb_data.apply(normalize_text)\n",
    "# Result already saved in \"normalized IMDB dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e894705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized train reviews\n",
    "norm_train_reviews=imdb_data.review[:40000]\n",
    "norm_test_reviews=imdb_data.review[40000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ee745",
   "metadata": {},
   "source": [
    "## Different Word Representations\n",
    "- Bag of Words\n",
    "- TF-IDF\n",
    "- Word2Vec - CBOW\n",
    "- Word2Vec - Skipgram\n",
    "- Glove\n",
    "- FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26cbbcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#labeling the sentient data\n",
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
    "print(sentiment_data.shape)\n",
    "\n",
    "#Spliting the sentiment data\n",
    "train_sentiments=sentiment_data[:40000]\n",
    "test_sentiments=sentiment_data[40000:]\n",
    "print(train_sentiments)\n",
    "print(test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46c7f43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (40000, 6209089)\n",
      "BOW_cv_test: (10000, 6209089)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(norm_test_reviews)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)\n",
    "#vocab=cv.get_feature_names()-toget feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "114a7e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (40000, 6209089)\n",
      "Tfidf_test: (10000, 6209089)\n"
     ]
    }
   ],
   "source": [
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(norm_test_reviews)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddafa0b2",
   "metadata": {},
   "source": [
    "### Word2Vec Details\n",
    "\n",
    "There are differet methods to get the sentence vectors :\n",
    "\n",
    "**Doc2Vec** : you can train your dataset using Doc2Vec and then use the sentence vectors.\n",
    "**Average of Word2Vec vectors** : You can just take the average of all the word vectors in a sentence. This average vector will represent your sentence vector.\n",
    "**Average of Word2Vec vectors with TF-IDF** : this is one of the best approach which I will recommend. Just take the word vectors and multiply it with their TF-IDF scores. Just take the average and it will represent your sentence vector.\n",
    "\n",
    "(reference: [link](https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence))\n",
    "\n",
    "\n",
    "Here, I used Doc2Vec approach using Gensim.\n",
    "In Gensim, there are two implementations for Doc2Vec:\n",
    "\n",
    "- Paragraph Vector - Distributed Memory (PV-DM) - corresponds to CBOW\n",
    "\n",
    "- Paragraph Vector - Distributed Bag of Words (PV-DBOW) - corresponds to SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33956dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "def generate_corpus(df, tokens_only = False):\n",
    "    for index, row in df.iteritems():\n",
    "        # print(row)\n",
    "        tokens = gensim.utils.simple_preprocess(row)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield doc2vec.TaggedDocument(tokens, [index])\n",
    "            \n",
    "train_corpus = list(generate_corpus(norm_train_reviews))\n",
    "test_corpus = list(generate_corpus(norm_test_reviews, tokens_only = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c18c44c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dm = 0: PV-DM\n",
    "# dm = 1: PV-DBOW\n",
    "d2v_model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=20, dm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3195dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4c6d2ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'good' appeared 23340 times in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word 'good' appeared {model.wv.get_vecattr('good', 'count')} times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ace31763",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8b3d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "[ 0.3115324   0.19853513  0.75726795  0.16892147 -0.38990209  0.11132143\n",
      " -0.38945508  0.13970904 -0.08977677 -0.17892691  0.2863491  -0.47374254\n",
      "  0.26724955  0.5288117   0.3347833  -0.205227    1.2687936   0.9980701\n",
      " -0.6963328  -0.03720236  0.32408106 -0.13349555  0.16489911 -0.2045367\n",
      "  0.96313477  0.5707606  -0.96842825 -0.44279906  0.09607672 -1.1550183\n",
      " -0.49319273 -0.509282    0.50498056  0.07007471  0.4714886   0.19841962\n",
      "  0.538801   -0.09300383 -0.45173794  0.04274365  1.4255229  -0.49658895\n",
      " -0.56747997  0.0910162   0.35376912 -0.19396082 -0.50346625  0.26528752\n",
      "  0.44515797  1.1060053 ]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "\n",
    "print(inferred_vector.shape)\n",
    "print(inferred_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf218d8",
   "metadata": {},
   "source": [
    "### GloVe Details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc4b61",
   "metadata": {},
   "source": [
    "## Models\n",
    "- Logistic Regression\n",
    "- Support Vector Machine\n",
    "- Multinomial Naive Bayes\n",
    "- RNN based structure\n",
    "- BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2773d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
